{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation\n",
    "\n",
    "[Live-demo by HuggingFace](https://transformer.huggingface.co/doc/gpt2-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My favourite movie is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'Ġfavourite', 'Ġmovie', 'Ġis']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoded = torch.tensor([tokenizer.encode(text)], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3666, 12507,  3807,   318]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = model(text_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 50257])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = predictions[0][:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50257])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ĠThe']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_text: str, tokens_to_generate: int):\n",
    "    text_generated = torch.tensor([tokenizer.encode(input_text)], dtype=torch.long)\n",
    "    result = text\n",
    "    with torch.no_grad():\n",
    "        for _ in range(tokens_to_generate):\n",
    "            predictions = model(text_generated)\n",
    "            next_token_logits = predictions[0][:, -1, :]\n",
    "\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "            \n",
    "            text_generated = torch.cat((text_generated, next_token_id), dim=1)\n",
    "            result = tokenizer.decode(text_generated.squeeze().tolist())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favourite movie is The Matrix, and I'm not sure if I'll ever watch it again\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(text, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favourite movie is The Matrix, and I'm not sure if I'll ever watch it again.\n",
      "\n",
      "I'm not sure if I'll ever watch it again. I'm not sure if I'll ever watch it again. I'm not sure if I'll ever\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(text, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Улучшение подходов генерации текста\n",
    "\n",
    "![image](https://miro.medium.com/max/1400/1*9sEpLZF8lV5OXwUQUMVZlg.png)\n",
    "\n",
    "### Сэмплирование с температурой\n",
    "\n",
    "Сэмплирование с температурой основана на статистической термодинамике, где высокая температура означает, что с большей вероятностью встречаются состояния с низкой энергией. В вероятностных моделях логиты играют роль энергии, и мы можем реализовать сэмплирование с температурой, разделив логиты на температуру перед подачей их в softmax и получением сэмплированных вероятностей.\n",
    "\n",
    "Более низкие температуры делают модель более уверенной в выборе, в то время как температуры выше 1 снижают уверенность. Температура 0 эквивалентна правдоподобию argmax/max, а бесконечная температура соответствует равномерной выборке.\n",
    "\n",
    "### Top K sampling\n",
    "\n",
    "Top-k сэмплирование означает сортировку по вероятности и обнуление вероятностей для всего, что ниже k-го токена. На практике получается так, что это улучшает качество за счет удаления хвоста и уменьшения вероятности отклонения от темы. Но в некоторых случаях действительно есть много слов, из которых мы могли бы разумно выбрать (широкое распределение ниже), а в некоторых случаях нет (узкое распределение ниже).\n",
    "\n",
    "![image](https://miro.medium.com/max/1400/0*J37qonVPJvKZpzv2)\n",
    "\n",
    "\n",
    "### Top P сэмплирование (nuclear sampling)\n",
    "\n",
    "\n",
    "Чтобы решить эту проблему, предлагаются использовать top p сэмплирование, также известную как nucleus sampling, в которой мы вычисляем кумулятивное распределение и отсекаем, как только [CDF](https://en.wikipedia.org/wiki/Cumulative_distribution_function) достигает P. В приведенном выше примере с широким распределением для достижения top_p = 0.9 может потребоваться больше 100 токенов. В узком распределении мы можем уже превысить top_p = 0.9 только с «hot» и «warm» в нашей выборке. Таким образом, мы по-прежнему избегаем выборки явно неправильных токенов, но сохраняем разнообразие, когда токены с наивысшей оценкой имеют слабую уверенность.\n",
    "Почему не работает выборка методом максимального правдоподобия? В процессе обучения никогда не бывает сложных ошибок. Модель обучена предсказывать следующий токен на основе контекста, созданного человеком. Если она ошибается с одним токеном из-за неправильного распределения, следующий токен использует «правильный» контекст, созданный человеком, независимо от последнего прогноза. Во время генерации он вынужден завершить свой собственный автоматически сгенерированный контекст, параметр, который он не учел во время обучения.\n",
    "\n",
    "[Статья с детальным описанием](https://arxiv.org/pdf/1904.09751.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-course",
   "language": "python",
   "name": "nlp-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
