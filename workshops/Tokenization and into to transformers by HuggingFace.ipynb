{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация\n",
    "\n",
    "Токенизация - обычная задача в обработке естественного языка (NLP). Это фундаментальный шаг как в традиционных методах NLP, таких как Count Vectorizer, так и в архитектурах на основе расширенного глубокого обучения, таких как Transformers.\n",
    "\n",
    "Токенизация - это способ разделения текста на более мелкие единицы, называемые токенами. Здесь токенами могут быть слова, символы или подслова. Следовательно, токенизацию можно в общих чертах разделить на 3 типа - токенизация слова, символа и подслова (n-граммовые символы).\n",
    "\n",
    "Самый распространенный способ формирования жетонов основан на \"пробеле\". Предполагая, что в качестве разделителя используется пробел, токенизация предложения приводит к 3 токенам - Never-give-up. Поскольку каждый токен представляет собой слово, он становится примером word-токенизации.\n",
    "\n",
    "Точно так же токены могут быть как символами, так и подсловами. Например, рассмотрим «smarter»:\n",
    "1. Character tokens: s-m-a-r-t-e-r\n",
    "2. Subword tokens: smart-er\n",
    "\n",
    "### Зачем делать токенизацию?\n",
    "\n",
    "Поскольку токены являются строительными блоками естественного языка, наиболее распространенный способ обработки сырого текста происходит на уровне токенов.\n",
    "\n",
    "Например, модели на основе Transformer - современные архитектуры глубокого обучения (SOTA) в NLP - обрабатывают сырой текст на уровне токена. Точно так же самые популярные архитектуры глубокого обучения для NLP, такие как RNN, GRU и LSTM, также обрабатывают необработанный текст на уровне токенов.\n",
    "\n",
    "![image](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/rnn.gif)\n",
    "\n",
    "Как показано здесь, RNN получает и обрабатывает каждый токен на определенном временном шаге.\n",
    "\n",
    "Следовательно, токенизация - это главный шаг при моделировании текстовых данных. Токенизация выполняется в корпусе для получения токенов. Следующие токены затем используются для подготовки словаря. Словарь - это набор уникальных токенов в корпусе. Помните, что словарь можно составить, рассматривая каждый уникальный токен в корпусе или рассматривая K наиболее часто встречающихся слов.\n",
    "\n",
    "**Создание словарного запаса - основная цель токенизации.**\n",
    "\n",
    "* Традиционные подходы NLP, такие как Count Vectorizer и TF-IDF, используют словарь в качестве признаков. Каждое слово в словаре рассматривается как уникальное свойство:\n",
    "\n",
    "![image](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/Screenshot-from-2020-05-21-12-46-42.png)\n",
    "\n",
    "* В архитектурах NLP на основе Deep Learning словарь используется для создания токенизированных входных предложений. Наконец, токены этих предложений передаются в качестве входных данных модели."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization\n",
    "\n",
    "Word Tokenization - это стандартный и самый простой алгоритм токенизации. Он разбивает фрагмент текста на отдельные слова на основе определенного разделителя. В зависимости от разделителей формируются разные токены уровня слова. Предварительно обученные эмбеддинги слов, такие как Word2Vec и GloVe, подпадают под токенизацию слов.\n",
    "\n",
    "**Недостатки токенизации по словам**:\n",
    "* слова вне словаря (OOV - out-of-vocabulary), которые появляются на тесте при обработке новых слов\n",
    "* большой словарь (при обучении мы используем много данных и чтобы минимизировать к-во oov слов, необходимо увеличивать размер словаря)\n",
    "\n",
    "### Char Tokenization\n",
    "\n",
    "Char Tokenization разбивает текст на набор символов. Это решает проблемы, которые мы видели выше о токенизации по словам.\n",
    "\n",
    "Char Tokenization последовательно обрабатывают слова OOV, сохраняя информацию о слове. Он разбивает слово OOV на символы и представляет слово в терминах этих символов.\n",
    "Это также ограничивает размер словарного запаса. \n",
    "\n",
    "**Недостатки токенизации по символам**:\n",
    "* данный подход решает проблему OOV, но длина входных и выходных предложений быстро увеличивается, поскольку мы представляем предложение как последовательность символов. В результате становится сложно изучить связи между символами для формирования значимых слов\n",
    "\n",
    "### Subword Tokenization\n",
    "\n",
    "Токенизация подслов разделяет фрагмент текста на подслова (или n-gram символы). Например, такие слова, как нижний, могут быть сегментированы как low-er, smartest, smart-est.\n",
    "\n",
    "В моделях на основе transformer - SOTA в NLP - используются алгоритмы токенизации подслов для подготовки словаря. Самый популярный из них – Byte Pair Encoding(BPE).\n",
    "\n",
    "### BPE\n",
    "\n",
    "Byte Pair Encoding (BPE) - широко используемый метод токенизации среди моделей на основе transformer. BPE решает проблемы токенизаторов слов и символов:\n",
    "\n",
    "* BPE эффективно борется с OOV. Он сегментирует OOV как подслова и представляет слово в терминах этих подслов\n",
    "* Длина предложений ввода и вывода после BPE короче по сравнению с токенизацией символов.\n",
    "\n",
    "BPE - это алгоритм сегментации слов, который итеративно объединяет наиболее часто встречающиеся символы или последовательности символов. \n",
    "\n",
    "Алгоритм по созданию BPE словаря:\n",
    "1. Разделить слова в корпусе на символы после добавления\n",
    "2. Инициализировать словарь с помощью уникальных символов в корпусе\n",
    "3. Вычислить частоту пар символов или последовательностей символов в корпусе\n",
    "4. Объединить наиболее часто встречающуюся пару\n",
    "5. Сохранить лучшую пару в словарь\n",
    "6. Повторить шаги с 3 по 5 для определенного количества итераций.\n",
    "\n",
    "Пример:\n",
    "\n",
    "У нас есть корпус: \n",
    "\n",
    "![image](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/Screenshot-from-2020-05-22-12-23-04.png)\n",
    "\n",
    "1a) Добавляем символ конца слова (скажем, </w>) к каждому слову в корпусе:\n",
    "\n",
    "![image](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/Screenshot-from-2020-05-22-12-31-05.png)\n",
    "\n",
    "1b) Токенизирием слова в корпусе на символы:\n",
    "\n",
    "![image](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/Screenshot-from-2020-05-22-12-44-12.png)\n",
    "\n",
    "2. Инициализируем словарь\n",
    "\n",
    "![image](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/Screenshot-from-2020-05-22-12-34-36-850x148.png)\n",
    "\n",
    "**Итерация 1**\n",
    "3. Считаем частоту пар символов\n",
    "\n",
    "![image](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/Screenshot-from-2020-05-22-12-53-53.png)\n",
    "\n",
    "4. Объединяем наиболее частую пару\n",
    "\n",
    "![image](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/Screenshot-from-2020-05-22-12-56-39-768x421.png)\n",
    "\n",
    "5. Сохраняем лучшую пару\n",
    "\n",
    "![image](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/Screenshot-from-2020-05-22-12-58-45-850x154.png)\n",
    "\n",
    "**Итерация 2**\n",
    "\n",
    "3. Считаем частоту пар символов\n",
    "\n",
    "![image](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/Screenshot-from-2020-05-22-13-19-02.png)\n",
    "\n",
    "4. Объединяем наиболее частую пару\n",
    "\n",
    "![image](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/Screenshot-from-2020-05-22-13-22-07-768x454.png)\n",
    "\n",
    "5. Сохраняем лучшую пару\n",
    "\n",
    "![image](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/Screenshot-from-2020-05-22-13-24-40-850x134.png)\n",
    "\n",
    "**После 10 итераций получим следующий результат**\n",
    "\n",
    "![image](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/Screenshot-from-2020-05-22-14-11-12-850x145.png)\n",
    "\n",
    "\n",
    "Применение BPE к OOV словам:\n",
    "1. После добавления разделяем OOV слово на символы </w>\n",
    "2. Вычисляем пары символов или последовательности символов в слове\n",
    "3. Выбираем пары, которые присутствуют в нашем словаре\n",
    "4. Объединяем самую частую пару\n",
    "5. Повторяйем шаги 2 и 3, пока не станет возможным объединение\n",
    "\n",
    "### BPE-dropout\n",
    "\n",
    "Отличие от стандартного BPE в том, что на этапе применения токенизации, мы с некой вероятностью *p* не выберем самую популярную пару, пропустим ее и проделаем такую же операцию со следующим кандидатом.\n",
    "\n",
    "![image](images/bpe_dropout.png)\n",
    "\n",
    "a) стандартный BPE\n",
    "b) dropout BPE (Дефисы указывают на возможные пары (пары, которые присутствуют в таблице пар); слияния, выполняемые на каждой итерации, показаны зеленым, dropout - красным.)\n",
    "\n",
    "Важное замечания, что dropout стоит использовать только при тренировке, для инференса нужно его отключать, чтобы всегда выбирать частые пары для которых мы должны были хорошо выучить эмбеддинги.\n",
    "\n",
    "**Преимущества**:\n",
    "* предотвращает переобучения на редких словах\n",
    "* выучивает определенные формы слова и учится \"составлять\" слова\n",
    "\n",
    "**Недостатки**:\n",
    "* не всегда данная регуляризация идет на пользу и обычно модель схватывает необходимые закономерности без применения dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤗 Transformers (раньше библиотека называлась pytorch-transformers и pytorch-pretrained-bert) предоставляет state-of-the-art архитектуры (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, T5, CTRL...) для Natural Language Understanding (NLU) и Natural Language Generation (NLG) с более тысячей предобученных моделей на 100+ языках и совместимостью PyTorch и TensorFlow 2.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фичи\n",
    "* Высокая производительность на NLU and NLG задачах\n",
    "* Низкий порог вход для студентов и практикующих специалистов\n",
    "\n",
    "State-of-the-art NLP для всех\n",
    "* Исследователи глубокого обучения\n",
    "* Практикующие специалисты\n",
    "* AI/ML/NLP преподаватели и студенты\n",
    "\n",
    "Более низкие затраты на вычисления\n",
    "* Исследователи могут делиться обученными моделями вместо того, чтобы постоянно заниматься переобучением\n",
    "* Практики могут сократить время вычислений и затраты в production\n",
    "* Десятки архитектур с более чем 1000 предобученных моделей, некоторые на более чем 100 языках\n",
    "\n",
    "Фреймворк для разных стадия жизненного цикла модели:\n",
    "* Тренировка state-of-the-art моделей в 3 строки кода\n",
    "* Глубокая совместимость TensorFlow 2.0 и PyTorch моделей\n",
    "* Переход на модели TF2.0/PyTorch по желанию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained models\n",
    "\n",
    "All pretrained models and their tokenizers could be found [here](https://huggingface.co/models).\n",
    "\n",
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I like to train deep learning models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer.encode(text, add_special_tokens=False)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text_with_padding = tokenizer.encode(text, add_special_tokens=False, pad_to_max_length=True, max_length=10)\n",
    "print(encoded_text_with_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab[\"I\"], tokenizer.vocab[\"i\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode_plus(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"I like to train deep learning models.\"\n",
    "text_2 = \"Unfortunately, my brother prefers playing computer games.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text_pair = tokenizer.encode(text_1, text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_text_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, weights in model.named_parameters():\n",
    "    print(name, weights.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-course",
   "language": "python",
   "name": "nlp-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
