{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor(data) создает объект torch.Tensor с заданными данными.\n",
    "vector_list = [1., 2., 3.]\n",
    "vector = torch.tensor(vector_list)\n",
    "print(vector)\n",
    "\n",
    "# Создает матрицу\n",
    "matrix_list = [[1., 2., 3.], [4., 5., 6]]\n",
    "matrix = torch.tensor(matrix_list)\n",
    "print(matrix)\n",
    "\n",
    "# Создает 3D тензор размера 2x2x2.\n",
    "tensor_list = [[[1., 2.], [3., 4.]],\n",
    "          [[5., 6.], [7., 8.]]]\n",
    "tensor = torch.tensor(tensor_list)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# По индексу из вектора можно получить значение в виде вектора (0 dimensional тензор)\n",
    "print(vector[0])\n",
    "# Получить python число\n",
    "print(vector[0].item())\n",
    "\n",
    "# По индексу из матрицы мы получим вектор\n",
    "print(matrix[0])\n",
    "\n",
    "# По индексу из тензора мы получим матрицу\n",
    "print(tensor[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы также можете создавать тензоры других типов данных. Чтобы создать тензор целочисленных типов, попробуйте `torch.tensor([[1, 2], [3, 4]])` (где все элементы должны быть типа integer). Вы также можете указать тип данных, передав `dtype=torch.data_type`. Обратитесь к документации, чтобы узнать о других типах данных, но наиболее распространенными будут Float и Long.\n",
    "\n",
    "Вы можете создать тензор со случайными данными и заданной размерностью с помощью `torch.randn()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((3, 4, 5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Операции с тензорами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor([4., 5., 6.])\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В [документации](https://pytorch.org/docs/stable/torch.html) можно найти полный список доступных операций. Они выходят за рамки просто математических операций.\n",
    "\n",
    "Одна полезная операция, которую мы будем использовать позже - это конкатенация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# По умолчанию он конкатенирует по первой оси (объединяет строки)\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "print(z_1)\n",
    "\n",
    "# Конкатенация колонок:\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "# second arg specifies which axis to concat along\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2)\n",
    "\n",
    "# Если ваши тензоры несовместимы, torch выкинет ошибку. Раскомментируйте, чтобы увидеть ошибку\n",
    "# torch.cat([x_1, x_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изменение формы тензоров\n",
    "\n",
    "Используйте .view() метод для изменения формы тензоров. Этот метод широко используется, потому что многие компоненты нейронной сети ожидают, что их входные данные будут иметь определенную форму. Часто вам нужно изменить форму перед передачей данных в компонент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12))  # Изменение формы в 2 строки и 12 колонок\n",
    "# Аналогичная операция ниже. Если одна из размерностей равна -1, то размер другой можно подобрать автоматически\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Граф вычислений и автоматическая дифференациация\n",
    "\n",
    "Концепция графа вычислений важна для эффективного программирования с глубоким обучением, поскольку позволяет не писать градиенты обратного распространения самостоятельно. Граф вычислений - это просто спецификация того, как ваши данные объединяются, чтобы дать вам результат. Поскольку граф полностью определяет, какие параметры были задействованы в каких операциях, он содержит достаточно информации для вычисления производных. Возможно, это звучит расплывчато, поэтому давайте посмотрим, что происходит, используя основной флаг `requires_grad`.\n",
    "\n",
    "Во-первых, подумайте с точки зрения программиста. Что хранится в объектах torch.Tensor, которые мы создавали выше? Очевидно, данные и форма, и, возможно, еще несколько вещей. Но когда мы сложили два тензора вместе, мы получили выходной тензор. Все, что знает этот выходной тензор, - это его данные и форма. Он понятия не имеет, что это была сумма двух других тензоров (она могла быть прочитана из файла, могла быть результатом какой-то другой операции и т. Д.)\n",
    "\n",
    "Если `requires_grad=True`, объект Tensor отслеживает, как он был создан. Давайте посмотрим на это в действии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# У фабричного метода Tensor есть флаг ``requires_grad``\n",
    "x = torch.tensor([1., 2., 3], requires_grad=True)\n",
    "\n",
    "# С requires_grad=True, вы по-прежнему можете выполнять все операции, которые вы ранее могли проводить\n",
    "y = torch.tensor([4., 5., 6], requires_grad=True)\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "# Но z знает немного больше\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значит, тензоры знают, что их создало. z знает, что это не было прочитано из файла, это не было результатом умножения, экспоненты или чего-то еще. И если вы продолжите следовать z.grad_fn, вы окажетесь в x и y.\n",
    "\n",
    "Но как это помогает нам вычислить градиент?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Давайте просуммируем все значения в z\n",
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, какова производная этой суммы по первой компоненте x? Исходя из математики мы хотим\n",
    "\n",
    "$$\\frac{\\partial s}{\\partial x_0}$$\n",
    "\n",
    "Что ж, s знает, что он был создан как сумма тензора z. z знает, что это была сумма x + y. Так\n",
    "\n",
    "$$s=x_0+y_0 + x_1+y_1 + x_2+y_2$$\n",
    "\n",
    "где $x_0 + y_0 = z_0$ и т.д.\n",
    "\n",
    "Таким образом, s содержит достаточно информации, чтобы определить, что нам нужна производная 1!\n",
    "\n",
    "Конечно, при этом игнорируется проблема того, как на самом деле вычислить эту производную. Дело здесь в том, что s несет достаточно информации, чтобы ее можно было вычислить. На самом деле разработчики Pytorch программируют операции sum() и +, чтобы знать, как вычислять их градиенты, и запускать алгоритм обратного распространения. Подробное обсуждение этого алгоритма выходит за рамки этого туториала.\n",
    "\n",
    "Пусть Pytorch вычислит градиент и убедится, что мы были правы: (обратите внимание, если вы запустите этот блок несколько раз, градиент будет увеличиваться. Это потому, что Pytorch накапливает градиент в свойстве .grad, поскольку для многих моделей это очень удобно .)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling .backward() on any variable will run backprop, starting from it.\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-course",
   "language": "python",
   "name": "nlp-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
