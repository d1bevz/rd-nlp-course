{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Базовые подходы к обработке текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В области обучения с учителем, например, для выполнения задач классификации, обычно наша цель - найти параметризованную модель, лучшую в своем классе.: <br><br> $$A(X, \\hat{w}): A(X, \\hat{w}) \\simeq f(X) \\Leftrightarrow A(X, \\hat{w}) = \\operatorname*{arg\\,min}_w \\left\\|A(X, w) - f(X)\\right\\|$$\n",
    "\n",
    "Где $X \\in R^{ n\\times m}$ - матрица фичей ($n$ наблюдений с $m$ фичами), $w \\in R^{m}$ - вектор параметров модели, $\\hat{w}$ - \"лучшие\" параметры модели\n",
    "\n",
    "Однако, как кандидат для X - все, что у нас есть - <strong>это сырой текст, алгоритмы не могут использовать его в таком виде.</strong>\n",
    "\n",
    "Чтобы применить машинное обучение к текстовым данным, нам сначала нужно преобразовать такой контент в некий числовой формат (чтобы сформировать векторы признаков). \n",
    "\n",
    "В обработке естественного языка автоматическое извлечение признаков может быть достигнуто разными способами: <strong>(bag-of-words, word embeddings, graph-based representations, etc.)</strong>\n",
    "\n",
    "Сегодня мы углубимся в детали подхода и методов <strong> bag-of-words </strong>, построенных на его основе в библиотеке Scikit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag-of-Words Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Intuition Behind the Model. Word Counters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рамках подхода bag-of-words мы работаем при следующих предположениях:\n",
    "* Текст можно анализировать без учета порядка слов / токенов\n",
    "* Нам нужно только знать, из каких слов / токенов состоит текст и сколько раз мы их встречали\n",
    "* Чем чаще слово / токен появляется в документе, тем он важнее\n",
    "Более формально, представленный набор текстов $T_1, T_2, ... , T_n$, мы извлекаем уникальные токены $w_1, w_2, ..., w_m$ чтобы сформировать словарь.\n",
    "\n",
    "Поэтому каждый текст $T_i$ представлен вектором фич $F_j = \\{x_{ij},\\ j \\in [1,m]\\}$, где $x_{ij}$ соотвествует числу соовстречаний со словом $w_j$ в тексте $T_i$\n",
    "\n",
    "Например, наш корпус состоит из **2 текстов**:\n",
    "\n",
    "[\"I love data science\", \n",
    "\"A data scientist is often smarter than a data analyst\"]\n",
    "\n",
    "\\* **На этапе предварительной обработки все буквы обычно приводят к нижнему регистру, иногда выполняется стемминг / лемматизация, а также удаление стоп-слов / знаков препинания, но это не обязательно.**\n",
    "\n",
    "Предположим, наши токены - простые униграммы (слова), поэтому мы можем выделить **11 уникальных слов**: : {i, love, data, science, a, scientist, is, often, smarter, than, analyst}\n",
    "\n",
    "Затем наш корпус сопоставляется с векторами признаков. $T_1=(1,1,1,1,0,0,0,0,0,0,0)$, $T_2=(0,0,2,0,2,1,1,1,1,1,1)$\n",
    "\n",
    "|Text #|i|love|data|science|a|scientist|is  |often|smarter|than|analyst|\n",
    "|------|------|------|------|------|------|------|------|------|------|------|------|\n",
    "|$T_1$|1|1|1|1|0|0|0|0|0|0|0|\n",
    "|$T_2$|0|0|2|0|2|1|1|1|1|1|1|\n",
    "\n",
    "Насколько эффективен этот подход с точки зрения памяти?\n",
    "Если n == 20k, этот текстовый корпус может породить словарь примерно с 100k элементами.\n",
    "<br> Таким образом, для хранения X в виде массива типа int32 потребуется 20000 x 100000 x 4 байта ~ **8 ГБ ОЗУ**, что с трудом возможно на современных компьютерах.\n",
    "К счастью, **большинство значений в X будут нулями**, поскольку для данного документа будет использовано менее пары тысяч (или даже сотен) отдельных слов. По этой причине мы говорим, что токены слов **обычно являются разреженными наборами данных большой размерности**. Мы можем сэкономить много памяти, сохраняя в памяти только ненулевые части векторов признаков.\n",
    "Разреженные матрицы - это структуры данных, которые делают именно это, и scikit-learn имеет встроенную поддержку этих структур.\n",
    "\n",
    "#### Преимущества\n",
    "* Очень интуитивно понятный подход, простой в использовании, понимании и применении - вы можете сами запрограммировать\n",
    "* Встроенная поддержка многих библиотек.\n",
    "* Разреженный формат с эффективным использованием памяти, приемлемый для большинства алгоритмов\n",
    "* Несмотря на простоту, работает хорошо, можно добиться хороших результатов\n",
    "* Быстрая предварительная обработка даже на 1 ядре\n",
    "\n",
    "#### Недостатки\n",
    "* Огромный корпус обычно приводит к огромному объему словарного запаса (миллионы слов), даже разряженный формат вам не поможет (только уловки хеширования)\n",
    "* Существуют другие подходы, позволяющие уловить больше деталей (семантика, отношения, структура) - вложения слов и т.д.\n",
    "* bag of words - это беспорядочное представление: отказ от пространственных отношений между функциями приводит к тому, что упрощенная модель не позволяет нам различать предложения, построенные из одних и тех же слов, но имеющие противоположные значения:\n",
    "    * \"New episodes **don't** feel like the first - watch it!\" (positive)\n",
    "    * \"New episodes feel like the first - **don't** watch it!\" (negative)\n",
    "* **Однако это можно отчасти исправить увеличением «длины» токена (unigrams $\\rightarrow$ bigrams, n-grams etc.), склеиванием отрицательных частиц со следующим словом (not like $\\rightarrow$ not_like), использованием character n-grams, skip-grams etc.** (смотрите [эту секцию для лучшего понимания n-gram](#3_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Улавливание зависимостей. N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель Bag-of-Words (BoW), построенная на простых токенах (униграммах), слишком упрощена и не улавливает пространственных зависимостей.\n",
    "Чтобы разобраться с этим и расширить наши знания, давайте вкратце вспомним, что такое **N-gram**:\n",
    "* N-gram это последовательность из $N$ токенов. \n",
    "* N-grams могут быть определенны по-разному, зависит от определения токенов. ('word', 'character', 'character_wb' etc.)\n",
    "\n",
    "1) **Word n-grams: (уловить больше семантики)** \n",
    "* unigrams: 'I love data science' $\\rightarrow$ [i, love, data, science]\n",
    "* bigrams (2-grams): 'I love data science' $\\rightarrow$ [i love, love data, data science]\n",
    "* 3-grams: 'I love data science' $\\rightarrow$ [i love data, love data science]\n",
    "* ...\n",
    "\n",
    "2) **Character n-grams: (позволяет ловить такие фичи, как \":)\", отчасти разобраться со словами с ошибками, такими как \"looong\" etc.)**\n",
    "* 5-grams: 'I love data science' $\\rightarrow$ [\"i lov\", \" love\", ... , \"cienc\", \"ience\"]\n",
    "* ...\n",
    "\n",
    "3) **Character-wb n-grams (n-grams, только в рамках слова):**\n",
    "* 5-grams: 'I love data science' $\\rightarrow$ {\" i \", \" love\", \"love \", ... , \"cienc\", \"ience\"]\n",
    "* ...\n",
    "\n",
    "4) **Skip-n-grams or k-skip-n-grams (оба character- и word-based, расширяет использование зависимостей)**\n",
    "* Последовательность из $N$ базовых токенов, имея расстояние из $\\leq K$ токенов между ними\n",
    "* 1-skip-2-grams: 'I love data science' $\\rightarrow$ [i data, love science]\n",
    "* ...\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#### Преимущества\n",
    "\n",
    "Тоже самое что и Bag-of-Words + улавливает больше контекста\n",
    "\n",
    "#### Недостатки\n",
    "\n",
    "Не забывайте, что с увеличением диапазона n-gram словарный запас **быстро растет**!\n",
    "<br>**|(1,1)-grams| << |(1,2)-grams| << |(1,3)-grams| << ...**\n",
    "<br>where (1,1)-grams = unigrams, (1,2)-grams = unigrams AND bigrams, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3  CountVectorizer\n",
    "\n",
    "CountVectorizer в Sklearn реализует вышеупомянутый подход Bag-of-Words:\n",
    "\n",
    "**Часто используемые параметры:**\n",
    "* **analyzer**={‘word’, ‘char’, ‘char_wb’} - какой вид токенов использовать (word, char-n-grams etc.)\n",
    "* **ngram_range**=(min_n, max_n) - какое число токенов объединять в н-граммы. Например, ngram_range=(1,2) $\\rightarrow$ использование unigrams и bigrams\n",
    "* **stop_words**={‘english’, list_of_words, or None} - какие стоп-слова фильтровать и нужно ли это делать\n",
    "* **vocabulary**={None, your_own_dictionary} - использовать ли данный словарь или строить его из извлеченных токенов\n",
    "* **max_features**={N, None} - для создания словаря, который рассматривает **топ-N** терминов, упорядоченных по частоте терминов (TF) в корпусе\n",
    "* **max_df** – при построении словаря игнорировать термины, частота которых в документе строго превышает заданный порог (стоп-слова, специфичные для корпуса)\n",
    "* **min_df** – при построении словаря игнорировать термины, частота которых в документе строго ниже заданного порога"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage example\n",
    "\n",
    "# import CountVectorizer from sklearn library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create CountVectorizer object\n",
    "cv = CountVectorizer(\n",
    "                    analyzer='word', # token = word\n",
    "                    ngram_range=(1,1), # only unigrams are used, (1,2) - unigrams/bigrams, ..., etc.\n",
    "                    stop_words=['my', 'stop', 'word', 'list'], # or stop_words='english'\n",
    "                    vocabulary=None, # or vocabulary=your_own_dictionary\n",
    "                    max_df=1.0, # don't filter words by their frequency\n",
    "                    max_features=6 # only top-6 words will be used as columns\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Мы будем использовать его в качестве примера для других методов извлечения признаков.\n",
    "# В качестве входящих данных вы можете использовать списки, массивы numpy, pandas DataFrames.\n",
    "texts = [\n",
    "    'nobody can stop me', # \"stop\" will be filtered by stop_words list\n",
    "    'word is a building blocks of a text', # \"word\" will be filtered by stop_words list\n",
    "    'I like doing feature extraction on text',\n",
    "    'I do not like digits in text like'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained feature matrix X:\n",
      "[[0 1 1 0 0 0]\n",
      " [0 0 0 0 1 1]\n",
      " [1 0 0 0 0 1]\n",
      " [2 0 0 1 0 1]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# применение CountVectorizer на текстовом корпусе\n",
    "transformed_texts_cv = cv.fit_transform(texts)\n",
    "# конвертация разряженной репрезентации в dense формат для исследования\n",
    "print('Obtained feature matrix X:')\n",
    "print(transformed_texts_cv.todense(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary:\n",
      "column index:0, token: like\n",
      "column index:1, token: me\n",
      "column index:2, token: nobody\n",
      "column index:3, token: not\n",
      "column index:4, token: of\n",
      "column index:5, token: text\n"
     ]
    }
   ],
   "source": [
    "# отобразить словарь (отсортированный по индексу колонки), чтобы увидить сопоставление индексов / столбцов и слов \n",
    "print('Dictionary:')\n",
    "for k,v in sorted(cv.vocabulary_.items(), reverse=False):\n",
    "    print('column index:{}, token: {}'.format(v,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New sentence (transformed):\n",
      "[[1 0 0 0 0 0]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# преобразовать новые предложения (having CountVectorizer trained)\n",
    "new_text = ['i like feature extraction very much'] \n",
    "new_transformed = cv.transform(new_text)\n",
    "# некоторые слова, как \"very\" и \"much\", не использовались при построение словаря, поэтому они были пропущенны\n",
    "print('\\nNew sentence (transformed):')\n",
    "print(new_transformed.todense(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 TF-IDF. TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В TF-IDF подходе (term frequency - inverse document frequency), в дополнение к обычной BoW-модели сделаны следующие дополнения:\n",
    "* Текст можно анализировать без учета порядка слов/токенов\n",
    "* Нам нужно только знать, из каких слов/токенов состоит текст и сколько раз мы их встречали\n",
    "* Чем чаще слово/токен появляется в документе, тем он важнее\n",
    "* **Если слово/токен появляется в документе, но редко встречается в других документах - это важно, и наоборот: <br>если оно часто встречается в большинстве документов - тогда мы не можем полагаться на это слово, чтобы помочь нам различать тексты**\n",
    "Таким образом, мы смотрим на весь корпус, обычные частоты слов (частоты терминов, TF) взвешиваются по множителю IDF:\n",
    "\n",
    "$$  \n",
    "    \\begin{cases} TF(w,T)=n_{Tw} \\\\ IDF(w, T)= log{\\frac{N}{n_{w}}}\\end{cases} \\implies \n",
    "    TF\\text{-}IDF(w, T) = n_{Tw}\\ log{\\frac{N}{n_{w}}} \\ \\ \\ \\ \\forall w \\in W\n",
    "$$\n",
    "\n",
    "<br> где $T$ соответствует количество документов (text), \n",
    "<br>$w$ - выбранное слово в документе T, \n",
    "<br>$n_{Tw}$ - число использований слова $w$ в тексте $T$, \n",
    "<br>$n_{w}$ - число документов содержащих слово $w$, \n",
    "<br> $N$ - общее число документов в корпусе.\n",
    "\n",
    "\n",
    "**Часто используемые параметры:**\n",
    "* **analyzer**={‘word’, ‘char’, ‘char_wb’} - какой вид токенов использовать (word, char-n-grams etc.)\n",
    "* **ngram_range**=(min_n, max_n) - какое число токенов объединять в н-граммы. Например, ngram_range=(1,2) $\\rightarrow$ использование unigrams и bigrams\n",
    "* **stop_words**={‘english’, list_of_words, or None} (default) - какие стоп-слова фильтровать и нужно ли это делать\n",
    "* **vocabulary**={None, your_own_dictionary} - использовать ли данный словарь или строить его из извлеченных токенов\n",
    "* **max_features**={N, None} - для создания словаря, который рассматривает **топ-N** терминов, упорядоченных по частоте терминов (TF) в корпусе\n",
    "* **norm**={‘l1’, ‘l2’ or None, optional} - норма для регуляризации ($L_2-$, $L_1-$ norms)\n",
    "* **smooth_idf**={True, False} Сглаживает веса idf, добавляя единицу к частоте документов, как если бы был замечен дополнительный документ, содержащий каждый термин в коллекции ровно один раз. Предотвращает нулевые деления.\n",
    "* **max_df** – при построении словаря игнорировать термины, частота которых в документе строго превышает заданный порог (стоп-слова, специфичные для корпуса)\n",
    "* **min_df** – при построении словаря игнорировать термины, частота которых в документе строго ниже заданного порога"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage example\n",
    "\n",
    "# import TfidfVectorizer from sklearn library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create TfidfVectorizer object\n",
    "tv = TfidfVectorizer(\n",
    "                    analyzer='word', # token = word\n",
    "                    ngram_range=(1,1), # only unigrams are used, (1,2) - unigrams/bigrams, ..., etc.\n",
    "                    stop_words=['my', 'stop', 'word', 'list'], # or stop_words='english'\n",
    "                    vocabulary=None, # or vocabulary=your_own_dictionary\n",
    "                    max_df=1.0, # don't filter words by their frequency\n",
    "                    max_features=6, # only top-6 words will be used as columns,\n",
    "                    smooth_idf=True,\n",
    "                    norm='l2' # euclidean norm используется по дефолту\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# применяем TfidfVectorizer к нашему корпусу\n",
    "transformed_texts_tv = tv.fit_transform(texts)\n",
    "# конвертация разряженной репрезентации в dense формат для исследования\n",
    "print('Obtained feature matrix X (see, L2-norm is used):')\n",
    "print(transformed_texts_tv.todense(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отобразить словарь (отсортированный по индексу колонки), чтобы увидить сопоставление индексов / столбцов и слов \n",
    "print('Dictionary:')\n",
    "for k,v in sorted(tv.vocabulary_.items(), reverse=False):\n",
    "    print('column index:{}, token: {}'.format(v,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразовать новые предложения (having CountVectorizer trained)\n",
    "new_text = ['i like extraction very much'] \n",
    "new_transformed = tv.transform(new_text)\n",
    "# некоторые слова, как \"very\" и \"much\", не использовались при построение словаря, поэтому они были пропущенны\n",
    "print('\\nNew sentence (transformed):')\n",
    "print(new_transformed.todense(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Hashes. HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хеш-функция - это любая функция, которая **может использоваться для сопоставления данных произвольного размера с данными фиксированного размера**. \n",
    "<br>Значения, возвращаемые хеш-функцией, называются хеш-значениями, хеш-кодами или просто хешами.\n",
    "<br>$f(X) \\rightarrow \\{0,N-1\\}:\\ f(X) = X\\  mod\\ N$, функция, которая отображает входящие данные в набор из $N$ \"корзин\", является примером хеш-функции:\n",
    "\n",
    "\n",
    "Например, $N = 2^k = 2^3 = 8$, then $\\ f(15)=15\\ mod \\ 8 = 7,\\ f(9)=9\\ mod \\ 8 = 1,\\ ...$\n",
    "\n",
    "В этой реализации векторизатора используется трюк с хешированием, чтобы найти сопоставление **имени строки токена** с **целочисленным индексом** функции.\n",
    "\n",
    "#### Преимущества:\n",
    "\n",
    "*  **Очень масштабируемая память для больших наборов данных**, поскольку нет необходимости хранить словарь в памяти\n",
    "* Быстро сериализовать/десериализовать, поскольку он не содержит состояния, кроме параметров конструктора\n",
    "* Может использоваться в потоковой передаче (частичное хэширования) и/или распараллеливаться, поскольку во время хэширования состояние не вычисляется\n",
    "* Может использоваться как «глупое» уменьшение размерности\n",
    "\n",
    "#### Недостатвки (vs Vectorizers со словарем в памяти): \n",
    "\n",
    "* Невозможно вычислить обратное преобразование (чтобы перейти от индексов функций к строковым именам функций), <br>которые **могут быть проблемой при попытках анализа, какие фичи наиболее важны для модели**.\n",
    "* Могут быть **коллизии**: разные токены могут быть сопоставлены с одним и тем же «bucket» (индекс фичи). \n",
    "<br>Однако на практике это редко является проблемой, если количество bucket'ов достаточно велико. (e.g. $2^{18}$ для задач текстовой классификации)\n",
    "\n",
    "\n",
    "\\* Используемая хеш-функция - это подписанная 32-разрядная версия Murmurhash3 (для тех кто в этом заинтересован :)  )\n",
    "\n",
    "**Часто используемые параметры:**\n",
    "* **analyzer**={‘word’, ‘char’, ‘char_wb’} - какой вид токенов использовать (word, char-n-grams etc.)\n",
    "* **ngram_range**=(min_n, max_n) - какое число токенов объединять в н-граммы. Например, ngram_range=(1,2) $\\rightarrow$ использование unigrams и bigrams\n",
    "* **stop_words**={‘english’, list_of_words, or None} (default) - какие стоп-слова фильтровать и нужно ли это делать\n",
    "* **n_features**={N} - сколько bucket'ов использовать\n",
    "* **norm**={‘l1’, ‘l2’ or None, optional} - норма для регуляризации ($L_2-$, $L_1-$ norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример использования\n",
    "\n",
    "# import HashingVectorizer from sklearn library\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# create HashingVectorizer object\n",
    "hv = HashingVectorizer(\n",
    "                    analyzer='word', # token = word\n",
    "                    ngram_range=(1,1), # only unigrams are used, (1,2) - unigrams/bigrams, ..., etc.\n",
    "                    stop_words=['my', 'stop', 'word', 'list'], # or stop_words='english'\n",
    "                    n_features=6, # only 6 bins will be used as columns, high probability of collisions!\n",
    "                    norm=None\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# применение HashingVectorizer на нашем корпусе\n",
    "transformed_texts_hv = hv.fit_transform(texts)\n",
    "# конвертация разряженной репрезентации в dense формат для исследования\n",
    "print('Obtained feature matrix X (see, no norm is used):')\n",
    "print(transformed_texts_hv.todense(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I see no dictionary ...\n",
    "print('Dictionary:')\n",
    "print('Oops, Hashing trick assumes no vocabulary will be used at all, online learning :)')\n",
    "print(\"However, we won't be able to do reverse transform and to get exact words :( \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# трансформация новых предложений (имея натренированный HashingVectorizer)\n",
    "new_text = ['i like extraction very much'] \n",
    "new_transformed = hv.transform(new_text)\n",
    "# некоторые слова, как \"very\" и \"much\", не использовались при построение словаря, поэтому они были пропущенны\n",
    "print('\\nNew sentence (transformed):')\n",
    "print(new_transformed.todense(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Going Beyond: Feature Engineering\n",
    "\n",
    "Обычно конкретный домен ведет к определенной информации, скрытой внутри ваших данных.\n",
    "Вам нужно извлечь как можно больше таких инсайтов.\n",
    "\n",
    "Например, если мы хотим натренировать модель по сентимент анализу (задачу классификации) на IMDB датасете (обзоры фильмов), и нам кажется, что **многие обзоры могут содержать явные пометки (скажем, в формате x/xx)**, поэтому мы должны это проверить и извлечь полезную настраиваемую функцию:\n",
    "\n",
    "[\"Average film, however, starring Matt Damon, 8/10\", 1] $\\rightarrow$ {\"8/10\"} $\\rightarrow$ 8/10=0.8 ~ 1 $\\rightarrow$ review is positive\n",
    "<br>[\"2/10, there is nothing to add\", 0] $\\rightarrow$ {\"2/10\"} $\\rightarrow$ 2/10=0.2 ~ 0 $\\rightarrow$ review is negative.\n",
    "\n",
    "Однако имейте в виду даты и выбросы (в отношении этой конкретной фичи) или что-то еще - всегда проверяйте свой код/регулярные выражения:\n",
    "\n",
    "Например, некоректный парсинг **'01/10/1999'** приведет к **{1/10, 10/1999} или {1/10}  ~ 0 (негативный отзыв?!)** ошибкам.\n",
    "\n",
    "### В дальнейшем мы обсудим особенности предметной области, но они не являются панацеей в целом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Token-based Level\n",
    "\n",
    "Нам нужно смотреть на токены (слова, сущности такие как улыбки и т.д.) и пытаться извлечь значимые фичи\n",
    "\n",
    "* позитивные смайлики\n",
    "* негативные смайлики\n",
    "* явная оценка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import Word, TextBlob\n",
    "import re # for regular expressions\n",
    "\n",
    "# download resources to be used by TextBlob wrapper (if not yet downloaded)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this implemenation does not deal with aforementioned cases, \n",
    "# to extract rating \"candidates\" in a text s\n",
    "def get_rate(s):\n",
    "    # searching for possible candidates\n",
    "    candidates = re.findall(r'(\\d{1,3}[\\\\|/]{1}\\d{1,2})', s)\n",
    "    rates = []\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            rates.append(eval(c)) # by the way, \"eval\" is a prime evil, it may lead you to the dark side :)\n",
    "            # instead, say, install sympy\n",
    "            # from sympy import sympify\n",
    "            # sympify(\"1*5/6*(7+8)\").evalf()\n",
    "        except SyntaxError:\n",
    "            pass\n",
    "        except ZeroDivisionError:\n",
    "            return 0\n",
    "    return np.mean(rates) if rates else -1 # if there is more than 1 value, calculate mean\n",
    "\n",
    "# bags of positive/negative smiles\n",
    "positive_smiles = set([\n",
    "\":‑)\",\":)\",\":-]\",\":]\",\":-3\",\":3\",\":->\",\":>\",\"8-)\",\"8)\",\":-}\",\":}\",\":o)\",\":c)\",\":^)\",\"=]\",\"=)\",\":‑D\",\":D\",\"8‑D\",\"8D\",\n",
    "\"x‑D\",\"xD\",\"X‑D\",\"XD\",\"=D\",\"=3\",\"B^D\",\":-))\",\";‑)\",\";)\",\"*-)\",\"*)\",\";‑]\",\";]\",\";^)\",\":‑,\",\";D\",\":‑P\",\":P\",\"X‑P\",\"XP\",\n",
    "\"x‑p\",\"xp\",\":‑p\",\":p\",\":‑Þ\",\":Þ\",\":‑þ\",\":þ\",\":‑b\",\":b\",\"d:\",\"=p\",\">:P\", \":'‑)\", \":')\",  \":-*\", \":*\", \":×\"\n",
    "])\n",
    "negative_smiles = set([\n",
    "\":‑(\",\":(\",\":‑c\",\":c\",\":‑<\",\":<\",\":‑[\",\":[\",\":-||\",\">:[\",\":{\",\":@\",\">:(\",\"D‑':\",\"D:<\",\"D:\",\"D8\",\"D;\",\"D=\",\"DX\",\":‑/\",\n",
    "\":/\",\":‑.\",'>:\\\\', \">:/\", \":\\\\\", \"=/\" ,\"=\\\\\", \":L\", \"=L\",\":S\",\":‑|\",\":|\",\"|‑O\",\"<:‑|\"\n",
    "])\n",
    "\n",
    "# function to extract token-level features from texts\n",
    "def get_token_level_features(texts, visualize=True):\n",
    "    \n",
    "    # assume texts = pd.Series with review text\n",
    "    print('extracting token-level features...')\n",
    "    tdf = pd.DataFrame()\n",
    "    tdf['text'] = texts # this is our review\n",
    "    \n",
    "    # 1. extract rating, like \"great film. 9/10\" will yield 0.9\n",
    "    tdf['rating'] = tdf['text'].apply(get_rate).fillna(-1) # rating (if found in review, else substitute NaN's by -1)\n",
    "\n",
    "    # 2. extract smiles and count positive/negative smiles per review\n",
    "    tdf['positive_smiles'] = tdf.text.apply(lambda s: len([x for x in s.split() if x in positive_smiles]))\n",
    "    tdf['negative_smiles'] = tdf.text.apply(lambda s: len([x for x in s.split() if x in negative_smiles]))\n",
    "    \n",
    "    if visualize:\n",
    "        # this is used for visual clarity, return pd.DataFrame\n",
    "        return tdf \n",
    "    else:\n",
    "        # get correct (and sparse) representation of feature matrix F\n",
    "        from scipy.sparse import csr_matrix \n",
    "        return csr_matrix(tdf[tdf.columns[1:]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sentence-based / Text-based Level\n",
    "\n",
    "Мы перешли на уровень предложения/текста.\n",
    "<br> Давайте посмотрим, какие фичи мы можем искать:\n",
    "* **Количество предложений** (текст нужно разбить на предложения, затем извлечь длину полученного списка)\n",
    "* **Количество восклицательных знаков** (целое число) или присутствие (boolean) - улавливание стресса, особенно если мы используем вероятностный вывод вместо бинарной классификации\n",
    "* **Подсчет вопросительных знаков** (целое число) или присутствие (boolean) - иногда помогает уловить сарказм.\n",
    "* **Количество слов в верхнем регистре** (длины > 1, чтобы исключить начало предложений) - напряжение текста, особенно если мы используем вероятностный вывод вместо бинарной классификации\n",
    "* **Контрастные спряжения**, например {'вместо', 'тем не менее', 'наоборот', 'с другой стороны'} - чтобы уловить возможные изменения настроения.\n",
    "\n",
    "Немного информации о текстовых \"краях\" - первое / последнее предложения в обзоре:\n",
    "* **\"полярность\" первого/последнего продложения**\n",
    "* **\"субъективность\" первого/последнего продложения**\n",
    "* **\"чистота\" первого/последнего продложения или всего набора предложений** - поймать изменения тональности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's continue...\n",
    "\n",
    "# контрастные соединения\n",
    "contrast_conj = set([\n",
    "'alternatively','anyway','but','by contrast','differ from','elsewhere','even so','however','in contrast','in fact',\n",
    "'in other respects','in spite of','in that respect','instead','nevertheless','on the contrary','on the other hand',\n",
    "'rather','though','whereas','yet'])\n",
    "\n",
    "# чтобы получить \"чистоту\" отзыва ~ показывает ту же тональность к обзору (~ 1) или изменение настроения (~ 0)\n",
    "def purity(sentences):\n",
    "    # получает полярность по всем предложениям\n",
    "    polarities = np.array([TextBlob(x).sentiment.polarity for x in sentences])\n",
    "    return polarities.sum() / np.abs(polarities).sum()\n",
    "\n",
    "# uppercase pattern\n",
    "uppercase_pattern = re.compile(r'(\\b[0-9]*[A-Z]+[0-9]*[A-Z]+[0-9]*\\b)')\n",
    "\n",
    "# регулярное выражение для разделения отзыва на предложения, вы можете использовать метод из textblob: TextBlob(x).sentences_\n",
    "sentence_splitter = re.compile('(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\!|\\?|\\.)\\s')\n",
    "# you can https://regex101.com/ for regex creation/checking, very convenient\n",
    "\n",
    "# feature engineering\n",
    "def get_text_level_features(texts, visualize=True):\n",
    "    # assume text = pd.Series with review text\n",
    "    print('extracting text-level features...')\n",
    "    tdf = pd.DataFrame()\n",
    "    tdf['text'] = texts # наш отзыв\n",
    "    tdf['sentences'] = tdf.text.apply(lambda s: re.split(sentence_splitter, s)) # разделяем на предложения\n",
    "    \n",
    "    tdf['sentence_cnt'] = tdf['sentences'].apply(len) # считаем предложения\n",
    "    tdf['exclamation_cnt'] = tdf.text.str.count('\\!') # считаем знаки восклицания\n",
    "    tdf['question_cnt'] = tdf.text.str.count('\\?') # считаем вопросильные знаки\n",
    "    \n",
    "    # uppercase words cnt (like HOLY JESUS!)\n",
    "    tdf['upper_word_cnt'] = tdf.text.apply(lambda s: len(re.findall(uppercase_pattern, s)))\n",
    "    \n",
    "    # contrast conjugations\n",
    "    tdf['contrast_conj_cnt'] = tdf.text.apply(lambda s: len([c for c in contrast_conj if c in s]))\n",
    "    \n",
    "    # polarity of 1st sentence\n",
    "    tdf['polarity_1st_sent'] = tdf.sentences.apply(lambda s: TextBlob(s[0]).sentiment.polarity)\n",
    "    # subjectivity of 1st sentence\n",
    "    tdf['subjectivity_1st_sent'] = tdf.sentences.apply(lambda s: TextBlob(s[0]).sentiment.subjectivity)\n",
    "    # polarity of last sentence\n",
    "    tdf['polarity_last_sent'] = tdf.sentences.apply(lambda s: TextBlob(s[-1]).sentiment.polarity)\n",
    "    # subjectivity of last sentence\n",
    "    tdf['subjectivity_last_sent'] = tdf.sentences.apply(lambda s: TextBlob(s[-1]).sentiment.subjectivity)\n",
    "    # subjectivity of review itself\n",
    "    tdf['polarity'] = tdf.text.apply(lambda s: TextBlob(s[-1]).sentiment.polarity)\n",
    "    # \"purity\" of review, |sum(sentence polarity) / sum(|sentence polarity|))|, ~ 1 is better, ~ 0 -> mixed\n",
    "    tdf['purity'] = tdf.sentences.apply(purity)\n",
    "    tdf['purity'].fillna(0, inplace=True)\n",
    "    \n",
    "    if visualize:\n",
    "        # для визуализации dataframe\n",
    "        return tdf \n",
    "    else:\n",
    "        # для разряженного представления\n",
    "        from scipy.sparse import csr_matrix \n",
    "        return csr_matrix(tdf[tdf.columns[2:]].values)\n",
    "\n",
    "### БУДЬТЕ ОСТОРОЖНЫ, если вы используете ЛИНЕЙНЫЕ МОДЕЛИ и имеете ЕЩЕ КОРОТКИЕ ОТЗЫВЫ (1 предложение),\n",
    "### tdf['subjectivity_1st_sent'] ~ tdf['subjectivity_last_sent'], то получите одинаковые значения, что приведет к неифнормативности фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test custom features:\n",
    "\n",
    "reviews = [\n",
    "    \"Waste of time :( 2/10 for the plot and 4/10 for acting!\",\n",
    "    'Awful film! Nobody can like it',\n",
    "    'Wow! Am I impressed?? TOTALLY :D',\n",
    "    '7/10'\n",
    "]\n",
    "\n",
    "# token-based\n",
    "token_lf = get_token_level_features(reviews)\n",
    "token_lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# token-based\n",
    "token_lf = get_text_level_features(reviews)\n",
    "token_lf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups()\n",
    "test = fetch_20newsgroups(subset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('clf', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(clf__C=[10, 1, 0.1, 0.01])\n",
    "grid_search = GridSearchCV(pipeline, params, scoring=\"accuracy\", cv=skf, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(train[\"data\"], train[\"target\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_, grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('clf', LogisticRegression(C=1)),\n",
    "])\n",
    "pipeline.fit(train[\"data\"], train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(test[\"data\"])\n",
    "accuracy_score(test[\"target\"], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test[\"target\"], predictions, target_names=test[\"target_names\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projector_course",
   "language": "python",
   "name": "projector_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
